# -*- coding: utf-8 -*-
"""BS4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17lIrxRWQwxX1P5r1f0IAiRZZVJRkBgtG
"""

import requests
from bs4 import BeautifulSoup

url = 'https://769audio.vn/dau-cd-denon-132.html'
response = requests.get(url)

import re

soup = BeautifulSoup(response.text, 'html.parser')

# Find the link with the '»' text
last_page_link = soup.find('a', string='»')
if last_page_link:
    href = last_page_link['href']
    # Use regex to extract the page number
    match = re.search(r'p=(\d+)', href)
    if match:
        last_page_number = int(match.group(1))
        print("Last page number:", last_page_number)
    else:
        print("No page number found in the link.")
else:
    print("Last page link not found.")
    last_page_number = 1  # default to 1 if no last page link is found

# Now fetch all product links from all pages
final_product_links = []
for page_number in range(1, last_page_number + 1):
    response = requests.get(f"{url}/p={page_number}")
    page_soup = BeautifulSoup(response.text, 'html.parser')

    # Select the product links; modify this selector based on your actual HTML structure
    product_links = page_soup.select('.item_product_bc .item-list > a')
    for link in product_links:
      # Extract the href attribute from each link element
      href = link.get('href')
      final_product_links.append(href)  # Append the href to the list

import requests
from bs4 import BeautifulSoup

def get_video_links(url):
    try:
        # Fetch the webpage content
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Locate the div with class "title_chitiet" and find the next sibling div
            target_div = soup.find("div", class_="title_chitiet")
            if not target_div:
                return "Target div not found."

            content_div = target_div.find_next_sibling("div")
            if not content_div:
                return "Content div not found."

            # Extract video URLs
            videos = [iframe['src'] for iframe in content_div.find_all('iframe')]
            video_links = ",".join(videos)

            return video_links
        else:
            return f"Failed to retrieve webpage, status code: {response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"An error occurred: {str(e)}"

# Example usage
# url = "https://769audio.vn/san-pham/2745/dan-karaoke-gia-dinh-jbl.html"
# print(get_video_links(url))

import requests
from bs4 import BeautifulSoup

def get_status(url):
    try:
        # Fetch the webpage content
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Search for the status label in the page
            status_label = soup.find('span', class_='label', string='Tình trạng ')

            # Check if the status label was found before proceeding
            if status_label:
                status = status_label.find_next('b')
                if status:
                    status = status.text.strip()
                return status
            else:
                return "Status label not found on the page."
        else:
            return f"Failed to retrieve webpage, status code: {response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"An error occurred: {str(e)}"

# Example usage
# url = "https://769audio.vn/san-pham/3018/dan-karaoke-gia-dinh-jbl-so-10-khuyen-mai.html"
# print(get_status(url))

# video_link_list = []

# for link in final_product_links:
#   video_link_list.append(get_video_links(link))

# # print(video_link_list[0:5])

status_list = []

for link in final_product_links:
  status_list.append(get_status(link))

print(status_list[0:5])

import pandas as pd
data = {
   'Link': final_product_links,
   'Status': status_list,
}
df = pd.DataFrame(data)

print(df.to_string())

df.to_csv('status.csv', index=False)

# from google.colab import drive
# drive.mount("/content/drive")

import gspread
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.auth import default
import csv

# Authenticate and create a client
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheets file
spreadsheet = gc.open("Data in 769audio")

worksheet = spreadsheet.worksheet("Đầu CD Denon")
data = worksheet.get_all_values()

import pandas as pd

main_df = pd.DataFrame(data[1:], columns=data[0])

# print(main_df.head())

import pandas as pd

# Merge the DataFrames on the link columns
merged_df = pd.merge(main_df, df, left_on='link sản phẩm', right_on='Link', how='left')

# Update the "Tình trạng" column with "status" from df where available
merged_df['Tình trang'] = merged_df['Status'].fillna(merged_df['Tình trang'])

# Drop the unnecessary columns
merged_df.drop(columns=['Link', 'Status'], inplace=True)

# Replace main_df with updated DataFrame
main_df = merged_df

# print(main_df.head())

# Create a new worksheet in this spreadsheet
worksheet_title = "New Đầu CD Denon"  # Name of the new sheet
rows, cols = 1000, 20  # Specify the size, adjust based on your DataFrame size
new_worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=rows, cols=cols)

# Assuming main_df is already defined and processed as per your earlier steps
# Convert DataFrame to a list of lists (including headers)
data_to_upload = [main_df.columns.tolist()] + main_df.values.tolist()

# Update the new worksheet with data
new_worksheet.update('A1', data_to_upload) # A1 means a starting cell

print(f"Data uploaded successfully to the new sheet '{worksheet_title}'.")

# import pandas as pd

# # Load the data
# data = pd.read_excel('sample_data/769audio_vn_dau_cd_denon.xlsx')

# # Group data by 'pro-href' and aggregate texts
# combined_data = data.groupby('pro-href').agg({
#     'pro': 'first',  # Assuming the product name does not change
#     'gia': 'first',  # You might want to handle prices differently if they vary
#     'anh-src': lambda x: ', '.join(x.dropna().unique()),
#     'gth': lambda x: ' '.join(x.dropna().unique()),
#     'ctiet': lambda x: ' '.join(x.dropna().unique()),
#     'act-src': lambda x: ', '.join(x.dropna().unique()),
#     'pagination': 'first'
# })

# # Save the combined data back to a CSV
# combined_data.to_csv('sample_data/combined_769audio_dau_cd_denon.csv')